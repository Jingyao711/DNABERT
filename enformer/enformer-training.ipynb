{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb_ShvB9E8yM"
      },
      "source": [
        "Copyright 2021 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXQjDxgdwUmW"
      },
      "source": [
        "This colab showcases training of the Enformer model published in\n",
        "\n",
        "**\"Effective gene expression prediction from sequence by integrating long-range interactions\"**\n",
        "\n",
        "Žiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R. Kelley\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AVkKjy3bh_A"
      },
      "source": [
        "## Steps\n",
        "\n",
        "- Setup tf.data.Dataset by directly accessing the Basenji2 data on GCS: `gs://basenji_barnyard/data`\n",
        "- Train the model for a few steps, alternating training on human and mouse data batches\n",
        "- Evaluate the model on human and mouse genomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM_PMOT-2Xhi"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqR7ol3rxrtM"
      },
      "source": [
        "**Start the colab kernel with GPU**: Runtime -> Change runtime type -> GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhjR7StI1tZn"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiDFm-a41tKW",
        "outputId": "07de1a7d-57bb-451b-9367-7e321e1c37af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (1.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (0.1.9)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (2.0.2)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (2.0.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree>=0.1.1->dm-sonnet) (25.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dm-sonnet tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CokqDsb-fxme"
      },
      "outputs": [],
      "source": [
        "# Get enformer source code\n",
        "!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py\n",
        "!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmffZS_306eb"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTGOLrbZxNHK",
        "outputId": "f798e819-faba-42cb-f9d1-bf6e7a8e3e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_ENABLE_GPU_GARBAGE_COLLECTION=false\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "# Make sure the GPU is enabled\n",
        "assert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU'\n",
        "\n",
        "# Easier debugging of OOM\n",
        "%env TF_ENABLE_GPU_GARBAGE_COLLECTION=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S9ywsUmT05C1"
      },
      "outputs": [],
      "source": [
        "import sonnet as snt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YUIbu0Xu1BnA"
      },
      "outputs": [],
      "source": [
        "assert snt.__version__.startswith('2.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PWzsyJddILcx",
        "outputId": "3d565a87-e8dd-4dde-b360-ffa08b49bc7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.19.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOhdaXG95eOl",
        "outputId": "6d08d331-df2e-4924-ddac-b4ebf149bf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  8 21:54:37 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPU colab has T4 with 16 GiB of memory\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xx--Nco09fN"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BbXyDdoShFzX"
      },
      "outputs": [],
      "source": [
        "import enformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MEb8OZli2Nbu"
      },
      "outputs": [],
      "source": [
        "# @title `get_targets(organism)`\n",
        "def get_targets(organism):\n",
        "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
        "  return pd.read_csv(targets_txt, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2BuZ2gmUbpXZ"
      },
      "outputs": [],
      "source": [
        "# @title `get_dataset(organism, subset, num_threads=8)`\n",
        "import glob\n",
        "import json\n",
        "import functools\n",
        "\n",
        "\n",
        "def organism_path(organism):\n",
        "  return os.path.join('gs://basenji_barnyard/data', organism)\n",
        "\n",
        "\n",
        "def get_dataset(organism, subset, num_threads=8):\n",
        "  metadata = get_metadata(organism)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
        "                                    compression_type='ZLIB',\n",
        "                                    num_parallel_reads=num_threads)\n",
        "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
        "                        num_parallel_calls=num_threads)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_metadata(organism):\n",
        "  # Keys:\n",
        "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
        "  # pool_width, crop_bp, target_length\n",
        "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
        "  with tf.io.gfile.GFile(path, 'r') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "\n",
        "def tfrecord_files(organism, subset):\n",
        "  # Sort the values by int(*).\n",
        "  return sorted(tf.io.gfile.glob(os.path.join(\n",
        "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
        "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
        "\n",
        "\n",
        "def deserialize(serialized_example, metadata):\n",
        "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
        "  feature_map = {\n",
        "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
        "      'target': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_example(serialized_example, feature_map)\n",
        "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
        "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
        "  sequence = tf.cast(sequence, tf.float32)\n",
        "\n",
        "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
        "  target = tf.reshape(target,\n",
        "                      (metadata['target_length'], metadata['num_targets']))\n",
        "  target = tf.cast(target, tf.float32)\n",
        "\n",
        "  return {'sequence': sequence,\n",
        "          'target': target}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzGRXfwV4tYH"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M_vr1mbl3jbD",
        "outputId": "7b9efbf7-f877-44b9-ffc3-67df28ab2c5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  genome   identifier  \\\n",
              "0      0       0  ENCFF833POA   \n",
              "1      1       0  ENCFF110QGM   \n",
              "2      2       0  ENCFF880MKD   \n",
              "3      3       0  ENCFF463ZLQ   \n",
              "4      4       0  ENCFF890OGQ   \n",
              "\n",
              "                                                file  clip  scale sum_stat  \\\n",
              "0  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "1  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "2  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "3  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "4  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "\n",
              "                                         description  \n",
              "0  DNASE:cerebellum male adult (27 years) and mal...  \n",
              "1  DNASE:frontal cortex male adult (27 years) and...  \n",
              "2                                      DNASE:chorion  \n",
              "3  DNASE:Ishikawa treated with 0.02% dimethyl sul...  \n",
              "4                                      DNASE:GM03348  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7091f715-91db-4117-b771-09828e94d81d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>genome</th>\n",
              "      <th>identifier</th>\n",
              "      <th>file</th>\n",
              "      <th>clip</th>\n",
              "      <th>scale</th>\n",
              "      <th>sum_stat</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF833POA</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:cerebellum male adult (27 years) and mal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF110QGM</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:frontal cortex male adult (27 years) and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF880MKD</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:chorion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF463ZLQ</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:Ishikawa treated with 0.02% dimethyl sul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF890OGQ</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:GM03348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7091f715-91db-4117-b771-09828e94d81d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7091f715-91db-4117-b771-09828e94d81d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7091f715-91db-4117-b771-09828e94d81d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0b5fe9e9-95d1-40b6-b0ca-3306fcf77b7d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0b5fe9e9-95d1-40b6-b0ca-3306fcf77b7d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0b5fe9e9-95d1-40b6-b0ca-3306fcf77b7d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_targets_human",
              "summary": "{\n  \"name\": \"df_targets_human\",\n  \"rows\": 5313,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1533,\n        \"min\": 0,\n        \"max\": 5312,\n        \"num_unique_values\": 5313,\n        \"samples\": [\n          1025,\n          3872,\n          2594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identifier\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5313,\n        \"samples\": [\n          \"ENCFF992ZMC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5313,\n        \"samples\": [\n          \"/home/drk/tillage/datasets/human/chip/encode/ENCSR000BQE/summary/ENCFF992ZMC.w5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clip\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 114,\n        \"min\": 32,\n        \"max\": 384,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"scale\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_stat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sum\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4925,\n        \"samples\": [\n          \"CHIP:H3K36me3:MM.1S\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_targets_human = get_targets('human')\n",
        "df_targets_human.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YDSKttXI4hMT",
        "outputId": "38ed2ced-8bef-4ca9-b7f7-bd330190385f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Error executing an HTTP request: HTTP response code 400 with body '{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Bucket is a requester pays bucket but no user project provided.\",\n    \"errors\": [\n      {\n        \"message\": \"Bucket is a requester pays bucket but no user project provided.\",\n        \"domain\": \"global\",\n        \"reason\": \"required\"\n      }\n    ]\n  }\n}\n'\n\t when reading metadata of gs://basenji_barnyard/data/human/statistics.json",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-919387396.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuman_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmouse_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mouse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhuman_mouse_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmouse_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1485664291.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(organism, subset, num_threads)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morganism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morganism\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n\u001b[1;32m     14\u001b[0m                                     \u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZLIB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1485664291.py\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(organism)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morganism_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morganism\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'statistics.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m\"\"\"Returns the size of the file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m   \"\"\"\n\u001b[0;32m--> 908\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mstat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mstat_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m   \"\"\"\n\u001b[0;32m--> 924\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Error executing an HTTP request: HTTP response code 400 with body '{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Bucket is a requester pays bucket but no user project provided.\",\n    \"errors\": [\n      {\n        \"message\": \"Bucket is a requester pays bucket but no user project provided.\",\n        \"domain\": \"global\",\n        \"reason\": \"required\"\n      }\n    ]\n  }\n}\n'\n\t when reading metadata of gs://basenji_barnyard/data/human/statistics.json"
          ]
        }
      ],
      "source": [
        "human_dataset = get_dataset('human', 'train').batch(1).repeat()\n",
        "mouse_dataset = get_dataset('mouse', 'train').batch(1).repeat()\n",
        "human_mouse_dataset = tf.data.Dataset.zip((human_dataset, mouse_dataset)).prefetch(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vx3116C7oFW"
      },
      "outputs": [],
      "source": [
        "it = iter(mouse_dataset)\n",
        "example = next(it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeztqJZ74ixT",
        "outputId": "39dc4051-5a19-4443-b6b0-bf6869faf5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human\n",
            "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 5313]), tf.float32)}\n",
            "mouse\n",
            "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 1643]), tf.float32)}\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "it = iter(human_mouse_dataset)\n",
        "example = next(it)\n",
        "for i in range(len(example)):\n",
        "  print(['human', 'mouse'][i])\n",
        "  print({k: (v.shape, v.dtype) for k,v in example[i].items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHHNHzFVbvTk"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U3hLJaUdZkG"
      },
      "outputs": [],
      "source": [
        "def create_step_function(model, optimizer):\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(batch, head, optimizer_clip_norm_global=0.2):\n",
        "    with tf.GradientTape() as tape:\n",
        "      outputs = model(batch['sequence'], is_training=True)[head]\n",
        "      loss = tf.reduce_mean(\n",
        "          tf.keras.losses.poisson(batch['target'], outputs))\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply(gradients, model.trainable_variables)\n",
        "\n",
        "    return loss\n",
        "  return train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXv5HU_242Ut"
      },
      "outputs": [],
      "source": [
        "learning_rate = tf.Variable(0., trainable=False, name='learning_rate')\n",
        "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
        "num_warmup_steps = 5000\n",
        "target_learning_rate = 0.0005\n",
        "\n",
        "model = enformer.Enformer(channels=1536 // 4,  # Use 4x fewer channels to train faster.\n",
        "                          num_heads=8,\n",
        "                          num_transformer_layers=11,\n",
        "                          pooling_type='max')\n",
        "\n",
        "train_step = create_step_function(model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrbDaOMWcFUl",
        "outputId": "6a42f69c-3003-47f2-a8d2-1b94c52eb57e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:24<00:00,  1.25s/it]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.774059 loss_mouse 0.94303024 learning_rate 2.0000002e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.0067647 loss_mouse 0.8752468 learning_rate 4.0000004e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.0471998 loss_mouse 0.89318746 learning_rate 6e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.010262 loss_mouse 1.02991 learning_rate 8.000001e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.111991 loss_mouse 0.84773445 learning_rate 1.0000001e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "steps_per_epoch = 20\n",
        "num_epochs = 5\n",
        "\n",
        "data_it = iter(human_mouse_dataset)\n",
        "global_step = 0\n",
        "for epoch_i in range(num_epochs):\n",
        "  for i in tqdm(range(steps_per_epoch)):\n",
        "    global_step += 1\n",
        "\n",
        "    if global_step > 1:\n",
        "      learning_rate_frac = tf.math.minimum(\n",
        "          1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))\n",
        "      learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
        "\n",
        "    batch_human, batch_mouse = next(data_it)\n",
        "\n",
        "    loss_human = train_step(batch=batch_human, head='human')\n",
        "    loss_mouse = train_step(batch=batch_mouse, head='mouse')\n",
        "\n",
        "  # End of epoch.\n",
        "  print('')\n",
        "  print('loss_human', loss_human.numpy(),\n",
        "        'loss_mouse', loss_mouse.numpy(),\n",
        "        'learning_rate', optimizer.learning_rate.numpy()\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs0f0z0RcCfz"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8c4lNQrHkXSC"
      },
      "outputs": [],
      "source": [
        "# @title `PearsonR` and `R2` metrics\n",
        "\n",
        "def _reduced_shape(shape, axis):\n",
        "  if axis is None:\n",
        "    return tf.TensorShape([])\n",
        "  return tf.TensorShape([d for i, d in enumerate(shape) if i not in axis])\n",
        "\n",
        "\n",
        "class CorrelationStats(tf.keras.metrics.Metric):\n",
        "  \"\"\"Contains shared code for PearsonR and R2.\"\"\"\n",
        "\n",
        "  def __init__(self, reduce_axis=None, name='pearsonr'):\n",
        "    \"\"\"Pearson correlation coefficient.\n",
        "\n",
        "    Args:\n",
        "      reduce_axis: Specifies over which axis to compute the correlation (say\n",
        "        (0, 1). If not specified, it will compute the correlation across the\n",
        "        whole tensor.\n",
        "      name: Metric name.\n",
        "    \"\"\"\n",
        "    super(CorrelationStats, self).__init__(name=name)\n",
        "    self._reduce_axis = reduce_axis\n",
        "    self._shape = None  # Specified in _initialize.\n",
        "\n",
        "  def _initialize(self, input_shape):\n",
        "    # Remaining dimensions after reducing over self._reduce_axis.\n",
        "    self._shape = _reduced_shape(input_shape, self._reduce_axis)\n",
        "\n",
        "    weight_kwargs = dict(shape=self._shape, initializer='zeros')\n",
        "    self._count = self.add_weight(name='count', **weight_kwargs)\n",
        "    self._product_sum = self.add_weight(name='product_sum', **weight_kwargs)\n",
        "    self._true_sum = self.add_weight(name='true_sum', **weight_kwargs)\n",
        "    self._true_squared_sum = self.add_weight(name='true_squared_sum',\n",
        "                                             **weight_kwargs)\n",
        "    self._pred_sum = self.add_weight(name='pred_sum', **weight_kwargs)\n",
        "    self._pred_squared_sum = self.add_weight(name='pred_squared_sum',\n",
        "                                             **weight_kwargs)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    \"\"\"Update the metric state.\n",
        "\n",
        "    Args:\n",
        "      y_true: Multi-dimensional float tensor [batch, ...] containing the ground\n",
        "        truth values.\n",
        "      y_pred: float tensor with the same shape as y_true containing predicted\n",
        "        values.\n",
        "      sample_weight: 1D tensor aligned with y_true batch dimension specifying\n",
        "        the weight of individual observations.\n",
        "    \"\"\"\n",
        "    if self._shape is None:\n",
        "      # Explicit initialization check.\n",
        "      self._initialize(y_true.shape)\n",
        "    y_true.shape.assert_is_compatible_with(y_pred.shape)\n",
        "    y_true = tf.cast(y_true, 'float32')\n",
        "    y_pred = tf.cast(y_pred, 'float32')\n",
        "\n",
        "    self._product_sum.assign_add(\n",
        "        tf.reduce_sum(y_true * y_pred, axis=self._reduce_axis))\n",
        "\n",
        "    self._true_sum.assign_add(\n",
        "        tf.reduce_sum(y_true, axis=self._reduce_axis))\n",
        "\n",
        "    self._true_squared_sum.assign_add(\n",
        "        tf.reduce_sum(tf.math.square(y_true), axis=self._reduce_axis))\n",
        "\n",
        "    self._pred_sum.assign_add(\n",
        "        tf.reduce_sum(y_pred, axis=self._reduce_axis))\n",
        "\n",
        "    self._pred_squared_sum.assign_add(\n",
        "        tf.reduce_sum(tf.math.square(y_pred), axis=self._reduce_axis))\n",
        "\n",
        "    self._count.assign_add(\n",
        "        tf.reduce_sum(tf.ones_like(y_true), axis=self._reduce_axis))\n",
        "\n",
        "  def result(self):\n",
        "    raise NotImplementedError('Must be implemented in subclasses.')\n",
        "\n",
        "  def reset_states(self):\n",
        "    if self._shape is not None:\n",
        "      tf.keras.backend.batch_set_value([(v, np.zeros(self._shape))\n",
        "                                        for v in self.variables])\n",
        "\n",
        "\n",
        "class PearsonR(CorrelationStats):\n",
        "  \"\"\"Pearson correlation coefficient.\n",
        "\n",
        "  Computed as:\n",
        "  ((x - x_avg) * (y - y_avg) / sqrt(Var[x] * Var[y])\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, reduce_axis=(0,), name='pearsonr'):\n",
        "    \"\"\"Pearson correlation coefficient.\n",
        "\n",
        "    Args:\n",
        "      reduce_axis: Specifies over which axis to compute the correlation.\n",
        "      name: Metric name.\n",
        "    \"\"\"\n",
        "    super(PearsonR, self).__init__(reduce_axis=reduce_axis,\n",
        "                                   name=name)\n",
        "\n",
        "  def result(self):\n",
        "    true_mean = self._true_sum / self._count\n",
        "    pred_mean = self._pred_sum / self._count\n",
        "\n",
        "    covariance = (self._product_sum\n",
        "                  - true_mean * self._pred_sum\n",
        "                  - pred_mean * self._true_sum\n",
        "                  + self._count * true_mean * pred_mean)\n",
        "\n",
        "    true_var = self._true_squared_sum - self._count * tf.math.square(true_mean)\n",
        "    pred_var = self._pred_squared_sum - self._count * tf.math.square(pred_mean)\n",
        "    tp_var = tf.math.sqrt(true_var) * tf.math.sqrt(pred_var)\n",
        "    correlation = covariance / tp_var\n",
        "\n",
        "    return correlation\n",
        "\n",
        "\n",
        "class R2(CorrelationStats):\n",
        "  \"\"\"R-squared  (fraction of explained variance).\"\"\"\n",
        "\n",
        "  def __init__(self, reduce_axis=None, name='R2'):\n",
        "    \"\"\"R-squared metric.\n",
        "\n",
        "    Args:\n",
        "      reduce_axis: Specifies over which axis to compute the correlation.\n",
        "      name: Metric name.\n",
        "    \"\"\"\n",
        "    super(R2, self).__init__(reduce_axis=reduce_axis,\n",
        "                             name=name)\n",
        "\n",
        "  def result(self):\n",
        "    true_mean = self._true_sum / self._count\n",
        "    total = self._true_squared_sum - self._count * tf.math.square(true_mean)\n",
        "    residuals = (self._pred_squared_sum - 2 * self._product_sum\n",
        "                 + self._true_squared_sum)\n",
        "\n",
        "    return tf.ones_like(residuals) - residuals / total\n",
        "\n",
        "\n",
        "class MetricDict:\n",
        "  def __init__(self, metrics):\n",
        "    self._metrics = metrics\n",
        "\n",
        "  def update_state(self, y_true, y_pred):\n",
        "    for k, metric in self._metrics.items():\n",
        "      metric.update_state(y_true, y_pred)\n",
        "\n",
        "  def result(self):\n",
        "    return {k: metric.result() for k, metric in self._metrics.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x80gX9LrhBU-"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset, head, max_steps=None):\n",
        "  metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
        "  @tf.function\n",
        "  def predict(x):\n",
        "    return model(x, is_training=False)[head]\n",
        "\n",
        "  for i, batch in tqdm(enumerate(dataset)):\n",
        "    if max_steps is not None and i > max_steps:\n",
        "      break\n",
        "    metric.update_state(batch['target'], predict(batch['sequence']))\n",
        "\n",
        "  return metric.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57fNitK9hzwd",
        "outputId": "947aaadb-dad2-4a00-ddac-d765f65d782f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:23,  6.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'PearsonR': 0.0028573992}\n"
          ]
        }
      ],
      "source": [
        "metrics_human = evaluate_model(model,\n",
        "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
        "                               head='human',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY_wj95xiDtE",
        "outputId": "fea839f7-b6c9-46ed-aece-c56b02e9ea16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:21,  6.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'PearsonR': 0.005183698}\n"
          ]
        }
      ],
      "source": [
        "metrics_mouse = evaluate_model(model,\n",
        "                               dataset=get_dataset('mouse', 'valid').batch(1).prefetch(2),\n",
        "                               head='mouse',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_mouse.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k1yaJrNCgvw"
      },
      "source": [
        "# Restore Checkpoint\n",
        "\n",
        "Note: For the TF-Hub Enformer model, the required input sequence length is 393,216 which actually gets cropped within the model to 196,608. The open source module does not internally crop the sequence. Therefore, the code below crops the central `196,608 bp` of the longer sequence to reproduce the output of the TF hub from the reloaded checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB2cGdH8EGfn"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "EXTENDED_SEQ_LENGTH = 393_216\n",
        "SEQ_LENGTH = 196_608\n",
        "inputs = np.array(np.random.random((1, EXTENDED_SEQ_LENGTH, 4)), dtype=np.float32)\n",
        "inputs_cropped = enformer.TargetLengthCrop1D(SEQ_LENGTH)(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdf35itsCjEY"
      },
      "outputs": [],
      "source": [
        "checkpoint_gs_path = 'gs://dm-enformer/models/enformer/sonnet_weights/*'\n",
        "checkpoint_path = '/tmp/enformer_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2P4IHqswLul",
        "outputId": "180abe21-ba00-4031-d9d7-2326f1f742f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/tmp/enformer_checkpoint’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /tmp/enformer_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTL8EISGCujC",
        "outputId": "2b743b9b-480d-44dc-b08e-82d2bc089a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://dm-enformer/models/enformer/sonnet_weights/checkpoint\n",
            "gs://dm-enformer/models/enformer/sonnet_weights/enformer-fine-tuned-human-1.data-00000-of-00001\n",
            "gs://dm-enformer/models/enformer/sonnet_weights/enformer-fine-tuned-human-1.index\n"
          ]
        }
      ],
      "source": [
        "# Copy checkpoints from GCS to temporary directory.\n",
        "# This will take a while as the checkpoint is ~ 1GB.\n",
        "for file_path in tf.io.gfile.glob(checkpoint_gs_path):\n",
        "  print(file_path)\n",
        "  file_name = os.path.basename(file_path)\n",
        "  tf.io.gfile.copy(file_path, f'{checkpoint_path}/{file_name}', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VSeTx0sCvcw",
        "outputId": "b52d7570-c355-4068-b932-3796b56e5586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 959M\n",
            "-rw-r--r-- 1 root root  111 May 25 10:58 checkpoint\n",
            "-rw-r--r-- 1 root root 959M May 25 10:59 enformer-fine-tuned-human-1.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root 5.7K May 25 10:59 enformer-fine-tuned-human-1.index\n"
          ]
        }
      ],
      "source": [
        "!ls -lh /tmp/enformer_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00Y2GgRED3aI"
      },
      "outputs": [],
      "source": [
        "enformer_model = enformer.Enformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFyIiGyiD5yh"
      },
      "outputs": [],
      "source": [
        "checkpoint = tf.train.Checkpoint(module=enformer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuyspnpOD9kA",
        "outputId": "3b495138-be27-4459-af82-c5da2af5bd2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/enformer_checkpoint/enformer-fine-tuned-human-1\n"
          ]
        }
      ],
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "print(latest)\n",
        "status = checkpoint.restore(latest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKkVOTyKEABJ"
      },
      "outputs": [],
      "source": [
        "# Using `is_training=False` to match TF-hub predict_on_batch function.\n",
        "restored_predictions = enformer_model(inputs_cropped, is_training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX650jqCEQdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "enformer_tf_hub_model = hub.load(\"https://tfhub.dev/deepmind/enformer/1\").model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiOTFTSdE5H1"
      },
      "outputs": [],
      "source": [
        "hub_predictions = enformer_tf_hub_model.predict_on_batch(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYrWgfaGFbpL",
        "outputId": "cb9d3ad0-ee14-46ac-b188-a4b6a697a159"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.allclose(hub_predictions['human'], restored_predictions['human'], atol=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wEOUMeNzK8q"
      },
      "outputs": [],
      "source": [
        "# Can run with 'is_training=True' but note that this will\n",
        "# change the predictions as the batch statistics will be updated\n",
        "# and the outputs will likley not match the TF-hub model.\n",
        "# enformer(inputs_cropped, is_training=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyVHRPAN5w6J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "enformer-training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}